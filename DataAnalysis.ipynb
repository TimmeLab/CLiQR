{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee49eeb-fd45-4df6-8eee-d4bf5c62f5e0",
   "metadata": {},
   "source": [
    "# Data Analysis Notebook\n",
    "\n",
    "This notebook contains code for an interactive GUI that can run the lick detection algorithm and display results. It is based on the MATLAB lickDetector.m script, and all code for the analysis algorithms is contained in data_analysis.py."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab9f43e-d85c-4187-85a0-c786cf0392ba",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24638f7d-9e43-4f65-85a4-a7164ed2bc34",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a353f7bf-726d-45a1-9102-1e8e0481a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "# Widget stuff\n",
    "#import ipywidgets as widgets\n",
    "import panel as pn\n",
    "pn.extension()\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lick detection algorithm\n",
    "from data_analysis import filter_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109a9f01-8feb-4fb2-a639-b0524c689c6d",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb4721d-fdd8-4c9c-8a9a-c45db7bb4f82",
   "metadata": {},
   "source": [
    "#### Experimental Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eeedaa-8ffc-485a-a275-e0f9c49ff3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental Parameters\n",
    "# Base directory that contains one folder per cohort (e.g., Lickometry Data/AEW2, Lickometry Data/AEW3, ...)\n",
    "base_dir = \"Lickometry Data\"\n",
    "\n",
    "# List all cohorts to include in this run\n",
    "#animal_id_prefixes = [\"AEW4\", \"AEW5\", \"AEW6\"]  # edit as needed\n",
    "animal_id_prefixes = [\"Example\"] # for single raw data file included in GitHub repo as example\n",
    "# Resolve cohort directories\n",
    "cohort_dirs = {p: os.path.join(base_dir, p) for p in animal_id_prefixes}\n",
    "\n",
    "# The recordings will be truncated to the start/stop times, then further trimmed at the end to be exactly\n",
    "# in accordance with this recording length parameter\n",
    "recording_length = 2 * 60 * 60  # 2 hours in seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769765af-cae9-4fa7-88e9-d3b19f750eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load layout files for each cohort (assumes each cohort folder contains layout.csv)\n",
    "layouts = {\n",
    "    p: pd.read_csv(os.path.join(cohort_dirs[p], \"layout.csv\"), header=None, index_col=0)\n",
    "    for p in animal_id_prefixes\n",
    "}\n",
    "\n",
    "# Union of all animal IDs across cohorts (columns in the final combined correlation)\n",
    "all_animal_ids = sorted({row[0] for df in layouts.values() for row in df.values})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a36a7a-fcb3-4078-9e40-231a2c947b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File picker GUI for selecting raw data files across multiple cohorts\n",
    "file_selectors = {\n",
    "    p: pn.widgets.FileSelector(\n",
    "        directory=cohort_dirs[p],\n",
    "        only_files=True,\n",
    "        file_pattern=\"*.h5\",\n",
    "    )\n",
    "    for p in animal_id_prefixes\n",
    "}\n",
    "\n",
    "def get_selected_files():\n",
    "    files = []\n",
    "    for _fs in file_selectors.values():\n",
    "        files.extend(list(_fs.value))\n",
    "    # preserve a deterministic ordering\n",
    "    return sorted(set(files))\n",
    "\n",
    "pn.Column(\n",
    "    \"Select raw data files (you can select files in multiple cohorts using the tabs below). \"\n",
    "    \"Use the '>>' button to add file(s) to the processing list. \"\n",
    "    \"Later cells read the current selections via get_selected_files().\",\n",
    "    pn.Tabs(*[(p, fs) for p, fs in file_selectors.items()]),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac08ec62-0995-4a94-b7a1-66961c363c0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Start/Stop Times Visualization\n",
    "If the script gives any warnings about start and stop times while running the analysis, it can be useful to plot the traces with start/stop times labeled. Then you can create time_fix.xlsx in the data directory to override the start/stop times stored in the raw data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c8dc90-79af-45ad-a85f-3f1bb470f5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time fixes\n",
    "#   An excel sheet can be used if there are errors in the start\n",
    "#   and stop times. The user will be able to override the start/stop times\n",
    "#   recorded and use the capacitance traces to identify start and stop\n",
    "#   times. This file will have three columns, with headers:\n",
    "#   Sensor, New Start Time, New End Time. The code will produce\n",
    "#   plots for the user and data tips can be used identify new start and\n",
    "#   stop times. The time fix file should be named the same base filename as\n",
    "#   the corresponding raw data, but with _time_fix.xlsx at the end (so\n",
    "#   raw_data_2026-01-20_11-10-18.h5 corresponds to\n",
    "#   raw_data_2026-01-20_11-10-18_time_fix.xlsx). Example time fix file format:\n",
    "#\n",
    "#   Sensor    New Start Time      New End Time\n",
    "#   20        1756825855          1756833056\n",
    "#   2         1757083015          1757090258\n",
    "#\n",
    "#   This would be time fixes for two recordings.\n",
    "#\n",
    "# To plot a capacitive trace, change the change the filename listed and the sensor number,\n",
    "# then re-run this cell. This is useful for determining start/stop times for the file.\n",
    "# The matplotlib widget backend has been enabled so that you can move around the graph and zoom.\n",
    "#\n",
    "# In general, this step can be skipped until a problem is encountered. The automated analysis will\n",
    "# account for accidental start/stops and use the last start_time/stop_time pair in the file. However,\n",
    "# if no start or stop time is recorded, this can be used to fix that situation.\n",
    "file = os.path.join(data_dir, \"raw_data_2026-01-20_11-10-18.h5\")\n",
    "sensor = 1\n",
    "\n",
    "# Determine which board did the recording based on sensor number\n",
    "match sensor:\n",
    "    case (1 | 2 | 3 | 7 | 8 | 9):\n",
    "        board = 0\n",
    "    case (4 | 5 | 6 | 10 | 11 | 12):\n",
    "        board = 1\n",
    "    case (13 | 14 | 15 | 19 | 20 | 21):\n",
    "        board = 2\n",
    "    case (16 | 17 | 18 | 22 | 23 | 24):\n",
    "        board = 3\n",
    "\n",
    "with h5py.File(file, 'r') as h5f:\n",
    "    sensor_data = h5f[f'board_FT232H{board}'][f'sensor_{sensor}']\n",
    "    cap_data = sensor_data['cap_data'][()]\n",
    "    time_data = sensor_data['time_data'][()]\n",
    "\n",
    "    # Determine which start time keys we have\n",
    "    pattern = re.compile(r'^start_time(\\d+)?$')\n",
    "    matches = {}\n",
    "    for k in sensor_data.keys():\n",
    "        m = pattern.match(k)\n",
    "        if m:\n",
    "            num = int(m.group(1)) if m.group(1) else -1\n",
    "            matches[num] = k\n",
    "    if matches:\n",
    "        num = -np.inf\n",
    "        for n in matches.keys():\n",
    "            if n > num: num = n\n",
    "        last_start = matches[num]\n",
    "        start_time = sensor_data[last_start][()]\n",
    "        # Try the stop_time corresponding to the start_time above\n",
    "        try:\n",
    "            stop_time = sensor_data['stop' + last_start[5:]][()]\n",
    "        except KeyError:\n",
    "            # Stop time wasn't recorded (likely clicked stop all button too soon)\n",
    "            stop_time = time_data[-1]\n",
    "    else: # no start time recorded\n",
    "        start_time = time_data[0] # start at beginning\n",
    "        stop_time = time_data[-1] # stop at end    \n",
    "\n",
    "total_recording_time = stop_time - start_time\n",
    "print(f\"Total recording length: {total_recording_time} seconds ({total_recording_time/60} minutes)\")\n",
    "start_idx = np.argmin(\n",
    "    np.abs(time_data - start_time)\n",
    ")\n",
    "stop_idx = np.argmin(\n",
    "    np.abs(time_data - stop_time)\n",
    ")\n",
    "\n",
    "%matplotlib widget\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(time_data, cap_data, zorder=0)\n",
    "ax.scatter(start_time, cap_data[start_idx], marker='o', c='r', s=50, zorder=1)\n",
    "ax.scatter(stop_time, cap_data[stop_idx], marker='o', c='r', s=50, zorder=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd39335e-2cc2-4657-8cd3-00da8ea46b69",
   "metadata": {},
   "source": [
    "## Run Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa8140-1a6d-4e20-bdb1-664244c0294c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through selected data files (across cohorts) and run the filter_data script on them\n",
    "selected_files = get_selected_files()\n",
    "\n",
    "missing_data_files = []\n",
    "for raw_data_filename in selected_files:\n",
    "    print(f\"Working on file: {raw_data_filename}\")\n",
    "\n",
    "    # Infer cohort prefix from the parent directory name (expects .../<PREFIX>/<raw_file>.h5)\n",
    "    prefix = os.path.basename(os.path.dirname(raw_data_filename))\n",
    "    if prefix not in layouts:\n",
    "        raise ValueError(f\"Cannot infer cohort prefix for file: {raw_data_filename} (got prefix='{prefix}').\")\n",
    "\n",
    "    layout = layouts[prefix]\n",
    "\n",
    "    raw_data_filename_base = os.path.basename(raw_data_filename)\n",
    "    filtered_data_filename_base = \"filtered\" + raw_data_filename_base[3:]\n",
    "    filtered_data_filename = os.path.join(os.path.dirname(raw_data_filename), filtered_data_filename_base)\n",
    "\n",
    "    time_fix_filename = raw_data_filename[:-3] + \"_time_fix.xlsx\"\n",
    "\n",
    "    with h5py.File(raw_data_filename, 'r') as raw_h5, h5py.File(filtered_data_filename, 'w') as filtered_h5:\n",
    "        try:\n",
    "            print(f\"Comments from current raw data file:\")\n",
    "            print(raw_h5['comments'][()] + \"\\n\")\n",
    "        except KeyError:\n",
    "            print(\"No comments in metadata\\n\")\n",
    "\n",
    "        # Check if there's a corresponding time fix file and pass it, if so\n",
    "        if os.path.exists(time_fix_filename):\n",
    "            missing_data = filter_data(\n",
    "                raw_h5,\n",
    "                filtered_h5,\n",
    "                layout,\n",
    "                raw_data_filename[:-3] + '.log',\n",
    "                time_fix=pd.read_excel(time_fix_filename, index_col=False, header=0),\n",
    "                algorithm=\"basic_threshold\",\n",
    "            )\n",
    "        else:\n",
    "            missing_data = filter_data(\n",
    "                raw_h5,\n",
    "                filtered_h5,\n",
    "                layout,\n",
    "                raw_data_filename[:-3] + '.log',\n",
    "                algorithm=\"basic_threshold\",\n",
    "            )\n",
    "\n",
    "    if missing_data:\n",
    "        missing_data_files.append(raw_data_filename)\n",
    "        print(f\"Skipping file {raw_data_filename} due to missing data\\n\")\n",
    "    else:\n",
    "        print(\"\")  # newline between files\n",
    "\n",
    "# Keep only files that had complete data\n",
    "selected_files = [f for f in selected_files if f not in set(missing_data_files)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b17399f-c3c8-4e82-a7ec-7fc1230e26df",
   "metadata": {},
   "source": [
    "## Combine Results by Animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a0b1f-7a82-457e-964b-4f4e47696c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dates of the raw data files, so we can get a list of dates analyzed to put in the combined filename\n",
    "dates = []\n",
    "date_range = []\n",
    "pattern = r'raw_data_(\\d{4}-\\d{2}-\\d{2})_'\n",
    "for raw_data_filename in selected_files:\n",
    "    match = re.search(pattern, os.path.basename(raw_data_filename))\n",
    "    if match:\n",
    "        dates.append(match.group(1))\n",
    "\n",
    "# Sort files/dates together by date to keep indexing consistent downstream\n",
    "if dates:\n",
    "    date_pairs = sorted(zip(dates, selected_files), key=lambda x: x[0])\n",
    "    dates, selected_files = map(list, zip(*date_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7d395e-eeb8-4fb8-8238-5c01e3a7cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the dates and create a compact date-range string for the combined filename\n",
    "parsed = [datetime.strptime(d, \"%Y-%m-%d\") for d in dates]\n",
    "\n",
    "year_tmp = parsed[0].year\n",
    "month_tmp = parsed[0].month\n",
    "\n",
    "# Add the first date in full to the date range\n",
    "date_range.append(parsed[0].strftime(\"%Y-%m-%d\"))\n",
    "for date in parsed[1:]:\n",
    "    # Same month and year\n",
    "    if date.year == year_tmp and date.month == month_tmp:\n",
    "        date_range.append(date.strftime(\"%d\"))\n",
    "    elif date.year == year_tmp:\n",
    "        month_tmp = date.month\n",
    "        date_range.append(date.strftime(\"%m-%d\"))\n",
    "    else:\n",
    "        year_tmp = date.year\n",
    "        month_tmp = date.month\n",
    "        date_range.append(date.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "# Join the dates to a string for the combined filename\n",
    "date_range = \"_\".join(date_range)\n",
    "\n",
    "# Combined filename includes all cohort prefixes\n",
    "prefix_str = \"-\".join(animal_id_prefixes)\n",
    "combined_filename = f\"results_combined_{prefix_str}_{date_range}.h5\"\n",
    "combined_filepath = os.path.join(base_dir, combined_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4811801d-d67f-49aa-864f-b84845443c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine results into a single H5 file grouped by animal_id, with a subgroup per input file (indexed by file number)\n",
    "\n",
    "# Precompute file -> cohort prefix mapping (expects .../<PREFIX>/<raw_file>.h5)\n",
    "file_info = []\n",
    "for f in selected_files:\n",
    "    prefix = os.path.basename(os.path.dirname(f))\n",
    "    if prefix not in layouts:\n",
    "        raise ValueError(f\"Cannot infer cohort prefix for file: {f} (got prefix='{prefix}').\")\n",
    "    file_info.append((f, prefix))\n",
    "\n",
    "# Re-create combined file each run\n",
    "with h5py.File(combined_filepath, 'w') as out_h5:\n",
    "    # Create top-level groups (one per animal across all cohorts)\n",
    "    for animal_id in all_animal_ids:\n",
    "        out_h5.create_group(animal_id)\n",
    "\n",
    "    # Populate per-file subgroups only for animals present in that cohort\n",
    "    for i, (raw_data_filename, prefix) in enumerate(file_info):\n",
    "        raw_data_filename_base = os.path.basename(raw_data_filename)\n",
    "        filtered_data_filename_base = \"filtered\" + raw_data_filename_base[3:]\n",
    "        filtered_data_filename = os.path.join(os.path.dirname(raw_data_filename), filtered_data_filename_base)\n",
    "\n",
    "        cohort_animal_ids = [row[0] for row in layouts[prefix].values]\n",
    "\n",
    "        with h5py.File(filtered_data_filename, 'r') as in_h5:\n",
    "            for animal_id in cohort_animal_ids:\n",
    "                if animal_id not in in_h5:\n",
    "                    continue  # animal missing from this file\n",
    "                # Create subgroup for this file index under this animal\n",
    "                grp = out_h5[animal_id]\n",
    "                grp2 = grp.create_group(str(i))\n",
    "\n",
    "                try:\n",
    "                    grp2.create_dataset('weights', data=in_h5[animal_id]['weight'][()])\n",
    "                    grp2.create_dataset('consumed_vols', data=in_h5[animal_id]['consumed_vol'][()])\n",
    "                except KeyError:\n",
    "                    # If weight or volume consumed wasn't properly recorded, skip this animal/file entry\n",
    "                    # (but keep others)\n",
    "                    del grp[str(i)]\n",
    "                    continue\n",
    "\n",
    "                grp2.create_dataset('lick_indices', data=in_h5[animal_id]['lick_indices'][()])\n",
    "                grp2.create_dataset('lick_times', data=in_h5[animal_id]['lick_times'][()])\n",
    "                grp2.create_dataset('cap_data', data=in_h5[animal_id]['cap_data'][()])\n",
    "                grp2.create_dataset('time_data', data=in_h5[animal_id]['time_data'][()])\n",
    "                grp2.create_dataset('used_start_idx', data=in_h5[animal_id]['used_start_idx'][()])\n",
    "                grp2.create_dataset('used_stop_idx', data=in_h5[animal_id]['used_stop_idx'][()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05638d50-5a56-4043-883a-742256812bb0",
   "metadata": {},
   "source": [
    "### Remove Intermediate Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c891de1-cd49-403b-96c1-a07694530752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove intermediate filtered_*.h5 files from all cohort folders (optional)\n",
    "for p in animal_id_prefixes:\n",
    "    for f in glob.glob(os.path.join(cohort_dirs[p], \"filtered_*.h5\")):\n",
    "        os.remove(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da33703-bf97-4a6c-8748-c32ccda16c48",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d374ca-6d88-4fc8-9d23-ab09d1a2e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the animal of interest and this cell will plot the full\n",
    "# capacitive trace and place a red dot on each detected lick.\n",
    "# This assumes you've selected raw data files and run the analysis so the corresponding\n",
    "# results_combined...h5 file exists and has its filename stored in combined_filename.\n",
    "\n",
    "animal_id = \"AEW4-3\"  # <-- set an explicit animal ID, e.g. \"AEW5-12\"\n",
    "\n",
    "with h5py.File(combined_filepath, 'r') as h5f:\n",
    "    animal_data = h5f[animal_id]\n",
    "    cap_data = {}\n",
    "    time_data = {}\n",
    "    lick_indices = {}\n",
    "    for grp_num, grp_data in animal_data.items():\n",
    "        grp_num = int(grp_num)\n",
    "        cap_data[grp_num] = grp_data['cap_data'][()]\n",
    "        time_data[grp_num] = grp_data['time_data'][()]\n",
    "        lick_indices[grp_num] = grp_data['lick_indices'][()]\n",
    "\n",
    "%matplotlib widget\n",
    "fig, ax = plt.subplots(nrows=len(cap_data), figsize=(10, 10))\n",
    "for i in range(len(cap_data)):\n",
    "    ax[i].plot(time_data[i], cap_data[i], zorder=0)\n",
    "    ax[i].scatter(time_data[i][lick_indices[i]], cap_data[i][lick_indices[i]], marker='o', c='r', s=15, zorder=1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f4cc4-c9d2-4129-8da4-6c78cc9fdd8b",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9ab441-aa3a-4e52-8544-481547001aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather arrays n_licks and vol_consumed (shape: n_files x n_animals) for correlation\n",
    "n_animals = len(all_animal_ids)\n",
    "n_files = len(selected_files)\n",
    "\n",
    "n_licks = np.full((n_files, n_animals), np.nan, dtype=float)\n",
    "vol_consumed = np.full((n_files, n_animals), np.nan, dtype=float)\n",
    "\n",
    "with h5py.File(combined_filepath, 'r') as h5f:\n",
    "    for a_idx, animal_id in enumerate(all_animal_ids):\n",
    "        if animal_id not in h5f:\n",
    "            continue\n",
    "        animal_data = h5f[animal_id]\n",
    "        for grp_num, grp_data in animal_data.items():\n",
    "            grp_num = int(grp_num)\n",
    "            if grp_num >= n_files:\n",
    "                continue\n",
    "            # consumed_vols is a scalar; lick_indices is a variable-length array\n",
    "            vol_consumed[grp_num, a_idx] = grp_data['consumed_vols'][()]\n",
    "            n_licks[grp_num, a_idx] = grp_data['lick_indices'][()].size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60676f82-2fdd-451b-b60a-e553bb5e9c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to replicate some MATLAB behavior from lickDetector.m\n",
    "def mad_based_outliers(r, thresh=3.0):\n",
    "    \"\"\"\n",
    "    Like MATLAB isoutlier, based on median absolute deviation\n",
    "    Flag points where |r - median(r)| > thresh * (1.4826 * MAD).\n",
    "    \"\"\"\n",
    "    r = np.asarray(r, dtype=float)\n",
    "    med = np.median(r)\n",
    "    mad = np.median(np.abs(r - med))\n",
    "    if mad == 0:\n",
    "        return np.zeros_like(r, dtype=bool)\n",
    "    sigma = 1.4826 * mad # from definition of MAD\n",
    "    return np.abs(r - med) > thresh * sigma\n",
    "\n",
    "\n",
    "def linear_index_to_row_col(idx0, n_files):\n",
    "    \"\"\"\n",
    "    MATLAB code:\n",
    "        exclude = [ceil(outlier/nFiles), rem(outlier,nFiles)];\n",
    "        exclude(exclude(:,2) == 0,2) = nFiles;\n",
    "    where outlier is 1-based.\n",
    "\n",
    "    Here idx0 is 0-based. Returns (row0, col0) 0-based indices matching\n",
    "    data[row][col][...].\n",
    "    \"\"\"\n",
    "    col0 = idx0 // n_files\n",
    "    row0 = idx0 % n_files\n",
    "    return row0, col0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f37fec0-b485-4e2b-a581-396d875368ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model (OLS)\n",
    "X = sm.add_constant(n_licks.ravel())\n",
    "y = vol_consumed.ravel()\n",
    "mdl = sm.OLS(y, X, missing='drop').fit()\n",
    "\n",
    "# Robust linear model (RLM)\n",
    "mdlr = sm.RLM(y, X, M=sm.robust.norms.HuberT(), missing='drop').fit()\n",
    "\n",
    "# Residuals from robust fit (raw residuals)\n",
    "yhat_r = mdlr.predict(X)\n",
    "resid_r = y - yhat_r\n",
    "\n",
    "# Residual probability plots\n",
    "plt.figure()\n",
    "stats.probplot(mdl.resid, dist=\"norm\", plot=plt)\n",
    "plt.title(f\"Linear Fit Residuals (Probability Plot), R^2 = {mdl.rsquared:.4f}\")\n",
    "\n",
    "plt.figure()\n",
    "stats.probplot(resid_r[~np.isnan(resid_r)], dist=\"norm\", plot=plt)\n",
    "plt.title(\"Robust Fit Residuals (Probability Plot)\")\n",
    "\n",
    "# Detect outliers in robust residuals\n",
    "# Note: resid_r may include NaNs if some animal/file entries were missing; drop them for outlier detection.\n",
    "valid_mask = ~np.isnan(resid_r)\n",
    "resid_valid = resid_r[valid_mask]\n",
    "is_out = mad_based_outliers(resid_valid, thresh=3.0)\n",
    "outlier_idx_valid0 = np.flatnonzero(is_out)  # indices into resid_valid\n",
    "\n",
    "# Map back to indices into the flattened full arrays\n",
    "flat_valid_idx0 = np.flatnonzero(valid_mask)\n",
    "outlier_idx0 = flat_valid_idx0[outlier_idx_valid0]\n",
    "\n",
    "# Show outliers to the user\n",
    "if outlier_idx0.size > 0:\n",
    "    plt.figure()\n",
    "    plt.scatter(n_licks.ravel(), y)\n",
    "    plt.scatter(n_licks.ravel()[outlier_idx0], y[outlier_idx0], marker=\"x\", c=\"r\")\n",
    "    plt.title(\"Outliers detected.\")\n",
    "    plt.xlabel(\"Number of Licks\")\n",
    "    plt.ylabel(\"Volume Consumed (mL)\")\n",
    "\n",
    "    # Reorganize outlier info into (row, col) indices for data access\n",
    "    # Here, col0 indexes all_animal_ids, row0 indexes selected_files\n",
    "    exclude = np.array([linear_index_to_row_col(i, n_files) for i in outlier_idx0], dtype=int)\n",
    "\n",
    "    # Plot capacitance traces for excluded channel-days\n",
    "    with h5py.File(combined_filepath, 'r') as h5f:\n",
    "        for row0, col0 in exclude:\n",
    "            file_num = row0\n",
    "            animal_id = all_animal_ids[col0]\n",
    "\n",
    "            # Some entries may not exist (e.g., missing data); skip if so\n",
    "            if animal_id not in h5f:\n",
    "                continue\n",
    "            animal_data = h5f[animal_id]\n",
    "            if str(file_num) not in animal_data:\n",
    "                continue\n",
    "\n",
    "            single_day_data = animal_data[str(file_num)]\n",
    "\n",
    "            plt.figure()\n",
    "            t = single_day_data['time_data'][()]\n",
    "            v = single_day_data['cap_data'][()]\n",
    "            lick_indices = single_day_data['lick_indices'][()]\n",
    "            plt.plot(t, v, zorder=0)\n",
    "            plt.scatter(t[lick_indices], v[lick_indices], marker='o', c='r', s=15, zorder=1)\n",
    "            plt.title(f\"Animal {animal_id}, date: {dates[file_num]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b86efa-db7b-4fd0-bfce-9ded4673ad11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
